{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Soham0410/Sentimental-Analysis/blob/main/Recurrent_Neural_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1gCwk1mzH0v",
        "outputId": "2ea781ac-28c3-4b96-939b-851375442ad3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.6.14)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.6.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.4)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow\n",
        "!pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNSfnn9JOLGn"
      },
      "outputs": [],
      "source": [
        "# Configuring the path of Kaggle.json file\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lp7JLb4H0Yge",
        "outputId": "e20b4aee-26a5-41e8-f35d-3d67338dde4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/kazanova/sentiment140\n",
            "License(s): other\n",
            "Downloading sentiment140.zip to /content\n",
            " 95% 77.0M/80.9M [00:00<00:00, 94.1MB/s]\n",
            "100% 80.9M/80.9M [00:00<00:00, 92.6MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Downloading the dataset\n",
        "!kaggle datasets download -d kazanova/sentiment140"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzn8424SzTdc",
        "outputId": "9141ec2d-13bd-4f2a-9c2e-f4957541bb07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The dataset is extracted\n"
          ]
        }
      ],
      "source": [
        "from zipfile import ZipFile\n",
        "dataset = 'sentiment140.zip'\n",
        "with ZipFile(dataset, 'r') as zip:\n",
        "  zip.extractall()\n",
        "  print('The dataset is extracted')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gb4XaWyh39s2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, f1_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFc2gHEF4LKG",
        "outputId": "ee3a673d-d316-41ef-8aa9-b96107bd9dc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# Download stopwords\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rouWNgJZ4bXd"
      },
      "outputs": [],
      "source": [
        "# Data processing\n",
        "socialmedia_data = pd.read_csv('training.1600000.processed.noemoticon.csv', encoding='ISO-8859-1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2yiAopG4oiO"
      },
      "outputs": [],
      "source": [
        "# Naming columns\n",
        "column_names = ['target', 'id', 'date', 'flag', 'user', 'text']\n",
        "socialmedia_data = pd.read_csv('training.1600000.processed.noemoticon.csv', names=column_names, encoding='ISO-8859-1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxmQbqEh5Ao9"
      },
      "outputs": [],
      "source": [
        "# Replace target values\n",
        "socialmedia_data.replace({'target': {4: 1}}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0occN1pA5CSV"
      },
      "outputs": [],
      "source": [
        "# Initialize PorterStemmer\n",
        "port_stem = PorterStemmer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RybmGLws6iKt"
      },
      "outputs": [],
      "source": [
        "# Stemming function\n",
        "def stemming(content):\n",
        "    stemmed_content = re.sub('[^a-zA-Z]', ' ', content)\n",
        "    stemmed_content = stemmed_content.lower()\n",
        "    stemmed_content = stemmed_content.split()\n",
        "    stemmed_content = [port_stem.stem(word) for word in stemmed_content if not word in stopwords.words('english')]\n",
        "    stemmed_content = ' '.join(stemmed_content)\n",
        "    return stemmed_content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dGauqCPU6o0F"
      },
      "outputs": [],
      "source": [
        "# Apply stemming\n",
        "socialmedia_data['stemmed_content'] = socialmedia_data['text'].apply(stemming)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HGvAlO357BbN"
      },
      "outputs": [],
      "source": [
        "# Separating the data and label\n",
        "X = socialmedia_data['stemmed_content']\n",
        "Y = socialmedia_data['target'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OoYyvcpS7CdL"
      },
      "outputs": [],
      "source": [
        "# Train-test split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, stratify=Y, random_state=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHlAdaUV8IkP"
      },
      "outputs": [],
      "source": [
        "# Tokenization and padding\n",
        "max_words = 10000\n",
        "max_len = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRjyXoJi8WYg"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wtkyRljv8jtC"
      },
      "outputs": [],
      "source": [
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NO826ZxW8mxK"
      },
      "outputs": [],
      "source": [
        "# Save the tokenizer\n",
        "with open('tokenizer.pkl', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vnJwlcik8pFD"
      },
      "outputs": [],
      "source": [
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eoc9VUjj8psC"
      },
      "outputs": [],
      "source": [
        "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len)\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1hcFj-et8tYD"
      },
      "outputs": [],
      "source": [
        "# Building the RNN model\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, 128, input_length=max_len))\n",
        "model.add(LSTM(128, return_sequences=True))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(LSTM(64))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWOs3RMF3h-9",
        "outputId": "c349b1b7-9a0c-4dba-b0e9-48459ecaf3e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "8000/8000 [==============================] - 5136s 641ms/step - loss: 0.4785 - accuracy: 0.7698 - val_loss: 0.4601 - val_accuracy: 0.7807\n",
            "Epoch 2/3\n",
            "8000/8000 [==============================] - 5249s 656ms/step - loss: 0.4525 - accuracy: 0.7862 - val_loss: 0.4542 - val_accuracy: 0.7843\n",
            "Epoch 3/3\n",
            "8000/8000 [==============================] - 5255s 657ms/step - loss: 0.4384 - accuracy: 0.7945 - val_loss: 0.4547 - val_accuracy: 0.7837\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "history = model.fit(X_train_pad, Y_train, epochs=3, batch_size=128, validation_split=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJ9uu5Dh4Ls7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3a55a4e-e457-4be9-b426-e0207e081d62"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "40000/40000 [==============================] - 3492s 87ms/step\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model on train data\n",
        "from sklearn.metrics import recall_score\n",
        "Y_train_pred_prob = model.predict(X_train_pad)\n",
        "Y_train_pred = (Y_train_pred_prob > 0.5).astype(\"int32\")\n",
        "\n",
        "train_accuracy = accuracy_score(Y_train, Y_train_pred)\n",
        "train_precision = precision_score(Y_train, Y_train_pred)\n",
        "train_recall = recall_score(Y_train, Y_train_pred)\n",
        "train_f1 = f1_score(Y_train, Y_train_pred)\n",
        "\n",
        "print(f'Accuracy Score on the training data: {train_accuracy}')\n",
        "print(f'Precision Score on the training data: {train_precision}')\n",
        "print(f'Recall Score on the training data: {train_recall}')\n",
        "print(f'F1 Score on the training data: {train_f1}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZ68B5MM3mq7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcb2c4dc-ab2a-4bf6-b10c-f938b07d1dc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000/10000 [==============================] - 916s 92ms/step\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model on  test data\n",
        "Y_pred_prob = model.predict(X_test_pad)\n",
        "Y_pred = (Y_pred_prob > 0.5).astype(\"int32\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9Eg86eo3qoy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb2f776b-a49f-4021-b361-9ebf2045357d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Score on the test data: 0.784121875\n",
            "Precision Score on the test data: 0.7664449198487824\n",
            "Recall Score on the test data: 0.81729375\n",
            "F1 Score on the test data: 0.7910530375209835\n"
          ]
        }
      ],
      "source": [
        "accuracy = accuracy_score(Y_test, Y_pred)\n",
        "precision = precision_score(Y_test, Y_pred)\n",
        "recall = recall_score(Y_test, Y_pred)\n",
        "f1 = f1_score(Y_test, Y_pred)\n",
        "\n",
        "print(f'Accuracy Score on the test data: {accuracy}')\n",
        "print(f'Precision Score on the test data: {precision}')\n",
        "print(f'Recall Score on the test data: {recall}')\n",
        "print(f'F1 Score on the test data: {f1}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2fgjlMt4UXU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "624c9996-2cc8-4539-9a83-4b4205e8b8ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "# Saving the trained model\n",
        "model.save('trained_rnn_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s96g0IEX4cfL"
      },
      "outputs": [],
      "source": [
        "# Load and use the trained model\n",
        "loaded_model = tf.keras.models.load_model('trained_rnn_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xK09IsKk4esE"
      },
      "outputs": [],
      "source": [
        "# Make predictions\n",
        "def predict_sentiment(text):\n",
        "    text = stemming(text)\n",
        "    seq = tokenizer.texts_to_sequences([text])\n",
        "    pad = pad_sequences(seq, maxlen=max_len)\n",
        "    pred = loaded_model.predict(pad)\n",
        "    return 'Positive Tweet' if pred[0][0] > 0.5 else 'Negative Tweet'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "StRHdyVa4gbi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0509d0f9-e6bd-4ec2-e29a-7a29ca05420c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 2s 2s/step\n",
            "Negative Tweet\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "Negative Tweet\n"
          ]
        }
      ],
      "source": [
        "# Example predictions\n",
        "print(predict_sentiment(X_test.iloc[1000]))\n",
        "print(predict_sentiment(X_test.iloc[3000]))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1mXbPjcR__Foh9d3vdviYlAzeATu5ImfK",
      "authorship_tag": "ABX9TyPK6Kc7gMBfL2/5PFH6qjzu",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}